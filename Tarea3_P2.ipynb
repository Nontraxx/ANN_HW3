{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parte 2 - IMDB\n",
    "## A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from theano.tensor.shared_randomstreams import RandomStreams\n",
    "from keras.datasets import imdb\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential,load_model_model_model\n",
    "from keras.layers import Dense, LSTM, Dropout\n",
    "from keras.layers.embeddings import Embedding\n",
    "import matplotlib.pyplot as plt\n",
    "np.random.seed(3)\n",
    "srng = RandomStreams(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(seed=15)\n",
    "X = np.concatenate((X_train, X_test), axis=0)\n",
    "y = np.concatenate((y_train, y_test), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review length: \n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAg0AAAFkCAYAAACjCwibAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAAPYQAAD2EBqD+naQAAGotJREFUeJzt3X+Q3XV97/HnOxoTwoWEkUs2TSnUKibximW3RMBG6cUS\ntN6tM1btYqZQbUdvberEoXZ07JXRO1fFH7ENeustKiq4dwTH20RtQuEqaEFps8jwYwOOl0QFs0qh\ni0MSw4/3/eN8dz17zI/P7tlzvns2z8fMmZPv9/P5fvPeP2Bf+Xw/n883MhNJkqSjWVB3AZIkqTcY\nGiRJUhFDgyRJKmJokCRJRQwNkiSpiKFBkiQVMTRIkqQihgZJklTE0CBJkooYGiRJUpFphYaIeGdE\n3B4Rj0XEWER8OSLOaOnzjYh4uunzVER8oqXPqRHx1Yh4PCL2RsQVEbGgpc/5EbEzIg5ExP0RccnM\nf0xJktSu6Y40rAO2AC8GXg4sBG6IiOOa+iTwv4DlQB+wAnjHRGMVDr4GPBM4B7gEuBR4b1Of04Gv\nADcBLwL+BrgqIn53mvVKkqRZEu28sCoiTgZ+Arw0M79Vnfs6cEdmvv0w17wC2AqsyMyHq3NvBj4A\n/MfMfDIiPgi8IjPPbLpuGFiama+cccGSJGnG2p3TsIzGyMIjLeffEBE/jYi7IuJ/tIxEnAPcNREY\nKjuApcALmvrc2HLPHcC5bdYrSZJm6JkzvTAiAvgY8K3MvLep6VpgD/AQcCZwBXAG8AdVex8w1nK7\nsaa2O4/Q58SIWJSZPz9EPc8G1gO7gQMz+6kkSTomLQZOB3Zk5r8drtOMQwPwCWAN8JLmk5l5VdPh\nPRGxF7gpIn49Mx84yj2P9KwkjtJnPY3AIkmSZuYNwBcO1zij0BARVwKvBNZl5o+P0v071fdzgQeA\nvcDZLX2WV997m76Xt/Q5BXgsMw8e5u/ZDXDNNdewevXqo5QkqdO2b9/O9u3bJ4+/+c1vsm7dusnj\niy66iIsuuqiO0iS1GB0dZcOGDVD9Lj2caYeGKjD8PvCyzPxBwSVn0RgdmAgXtwHvioiTm+Y1XAiM\nA6NNfV7Rcp8Lq/OHcwBg9erV9Pf3F5QlqZP6+/t517veNXnc19fHLbfcUmNFkgoc8fH+dPdp+ASN\noYuLgccjYnn1WVy1Pyci3h0R/RFxWkQMAp8Fbs7Mu6vb3ADcC3w+Is6MiPXA+4ArM/OJqs/fAb8R\nER+MiOdHxJ/RmBPx0enUK0mSZs90V0+8BTgR+AaNiY4Tn9dV7Qdp7N+wg8aowYeA64DBiRtk5tPA\nq4CngFuBzwFXA+9p6rMb+L3qXt8FNgFvyszWFRWSJKlLpvV4IjOPGDIy80fA+QX3+SGN4HCkPjcD\nA9OpT9LctXLlyrpLkNQm3z0hqSsuu+yyukuQ1CZDg6SuGBoaqrsESW0yNEiSpCKGBkmSVMTQIEmS\nihgaJElSEUODJEkqYmiQJElFDA2SJKmIoUGSJBUxNEiSpCKGBkmSVMTQIEmSihgaJHXF8PBw3SVI\napOhQVJXGBqk3mdokCRJRQwNkiSpyDPrLkDS/DQ8PDzlkcS2bdsYHBycPB4aGmJoaKiO0iTNkKFB\nUke0hoLBwUG2bt1aY0WS2uXjCUmSVMTQIEmSihgaJHWF8xek3mdokNQVhgap9xkaJElSEUODJEkq\nYmiQJElFDA2SJKmIoUGSJBUxNEiSpCKGBkmSVMTQIEmSihgaJElSEUODJEkqYmiQJElFDA2SJKmI\noUGSJBUxNEiSpCKGBkmSVMTQIKkrhoeH6y5BUpsMDZK64sMf/nDdJUhqk6FBUlc8+OCDdZcgqU2G\nBkmSVOSZdRcgaX4aHh6eMo9hbGyMwcHByeOhoSGGhobqKE3SDDnSIEmSijjSIKkjWkcS+vr62Lp1\na40VSWqXIw2SJKmIoUFSV6xcubLuEiS1ydAgqSsuu+yyukuQ1CZDg6SucKWE1PsMDZIkqYihQZIk\nFZlWaIiId0bE7RHxWESMRcSXI+KMlj6LIuLjEfFwRPwsIq6PiFNa+pwaEV+NiMcjYm9EXBERC1r6\nnB8ROyPiQETcHxGXzPzHlCRJ7ZruSMM6YAvwYuDlwELghog4rqnPx4DfA14DvBT4FeBLE41VOPga\njT0izgEuAS4F3tvU53TgK8BNwIuAvwGuiojfnWa9kiRplkxrc6fMfGXzcURcCvwEGAC+FREnAm8E\n/jAzb676/DEwGhFrM/N2YD2wCvidzHwYuCsi/hr4QERcnplPAv8V+H+Z+Y7qr7ovIn4b2AT80wx/\nVkmS1IZ25zQsAxJ4pDoeoBFEbprokJn3AT8Azq1OnQPcVQWGCTuApcALmvrc2PJ37Wi6hyRJ6rIZ\nh4aICBqPIr6VmfdWp/uAg5n5WEv3saptos/YIdop6HNiRCyaac2SJGnm2nn3xCeANcBvF/QNGiMS\nR3OkPlHQh02bNrF06dIp53ybniRJDa1voAUYHx8vunZGoSEirgReCazLzIeamvYCz4qIE1tGG07h\nFyMHe4GzW265vKlt4nt5S59TgMcy8+CRatu8eTP9/f1lP4gkSceYQ/1DemRkhIGBgaNeO+3HE1Vg\n+H0aExl/0NK8E3gSuKCp/xnArwG3VqduA14YESc3XXchMA6MNvW5gKkurM5LkqQaTGukISI+AQwB\ng8DjETExGjCemQcy87GI+BTw0Yh4FPgZ8LfAP2fmv1R9bwDuBT4fEX8FrADeB1yZmU9Uff4O+POI\n+CDwaRoB4g9ojG5IkqQaTHek4S3AicA3gIeaPq9r6rOJxh4L1zf1e81EY2Y+DbwKeIrG6MPngKuB\n9zT12U1jr4eXA9+t7vmmzGxdUSFJkrpkuvs0HDVkZObPgY3V53B9fkgjOBzpPjfTWMIpSZLmAN89\nIUmSihgaJElSEUODJEkqYmiQJElFDA2SJKmIoUGSJBUxNEiSpCKGBkmSVMTQIEmSihgaJHXFxo2H\n3SRWUo8wNEjqiuuuu67uEiS1ydAgSZKKGBokSVIRQ4Okjti4cSN9fX2Tn7GxsSnHznGQes+0Xo0t\nSaW2bNnCli1bJo/7+vrYu3dvjRVJapcjDZIkqYihQZIkFTE0SOqK1772tXWXIKlNhgZJXdE8v0FS\nbzI0SJKkIoYGSZJUxNAgSZKKGBokSVIRQ4MkSSpiaJDUFcPDw3WXIKlNhgZJXWFokHqfoUGSJBUx\nNEiSpCK+5VJSRwwPD095JLFt2zYGBwcnj4eGhhgaGqqjNEkzZGiQ1BGtoaCvr4+tW7fWWJGkdvl4\nQpIkFTE0SJKkIj6ekNQRrXMaxsbGnNMg9ThDg6SOaA0Fg4ODzmmQepyPJyRJUhFDgyRJKmJokNQV\np512Wt0lSGqToUFSV+zZs6fuEiS1ydAgSZKKGBokSVIRl1xK6gjfPSHNP4YGSR3hPg3S/OPjCUmS\nVMTQIEmSihgaJHWF+zRIvc/QIKkr3KdB6n2GBkmSVMTQIKkrHnzwwbpLkNQml1xK6ojWfRpGRkbc\np0HqcYYGSR3RGgqWLVvmPg1Sj/PxhKSu2L9/f90lSGrTtEcaImId8JfAALACeHVmbm1q/wxwSctl\n2zPzlU19TgKuBF4FPA18CXhbZj7e1OfMqs/ZwE+AKzPzQ9OtV1I9Wh9PHDx40McTUo+byeOJ44Hv\nAp+m8cv+UP4RuBSI6vjnLe1fAJYDFwDPAq4GPglsAIiIE4AdwA3Am4EXAp+JiEcz86oZ1Cypy3w8\nIc0/0w4Nmbkd2A4QEXGYbj/PzJ8eqiEiVgHrgYHMvKM6txH4akRclpl7aYSHhcCbMvNJYDQizgLe\nDhgapB7QOtIwPj7uSIPU4zo1EfL8iBgDHgX+L/DuzHykajsXeHQiMFRuBBJ4MfAPwDnALVVgmLAD\neEdELM3M8Q7VLWmWtIaCvr4+RxqkHteJiZD/CPwR8J+BdwAvA77WNCrRR2OOwqTMfAp4pGqb6DPW\nct+xpjZJPWblypV1lyCpTbM+0pCZX2w6vCci7gK+D5wPfP0IlwaN0YYjtXOUPmzatImlS5dOOecw\nqFQ/Q4M0N7Q+OoTG48MSHd+nITMfiIiHgefSCA17gVOa+0TEM4CTqjaq7+Utt5q4pnUEYorNmzfT\n39/fbtmSZpnBXZobDvUP6ZGREQYGBo56bcf3aYiIXwWeDfy4OnUbsKya2DjhAhojCbc39XlpFSYm\nXAjc53wGqTcZGqTeN+3QEBHHR8SLIuI3q1PPqY5PrdquiIgXR8RpEXEB8H+A+2lMZCQzd1V//vuI\nODsiXgJsAYarlRPQWJJ5EPh0RKyJiNcDfwF8pK2fVpIkzdhMHk/8Fo3HDFl9Jn6Rfxb4M+BMGhMh\nlwEP0QgI/y0zn2i6x8U0Nm66kcbmTtcDb5tozMzHImJ91edfgYeByzPzUzOoV5IkzYKZ7NNwM0ce\nobio4B7/TrWR0xH63EVj5YUkSZoDfPeEJEkqYmiQJElFDA2SumLjxo11lyCpTYYGSV1x3XXX1V2C\npDYZGiR1xf79++suQVKbDA2SuuLAgQN1lyCpTYYGSR2xceNG+vr6Jj8HDx6ccuwcB6n3dPzdE5KO\nTVu2bGHLli2TxwsWLGDv3r1HuELSXGdokNQRrW/Sy0wGBwcnj337rNR7DA2SOqI1FCxYsICtW7fW\nWJGkdhkaJHWEIw3S/GNokNQRraFg2bJljjRIPc7VE5IkqYihQVJXLF68uO4SJLXJ0CCpK1auXFl3\nCZLa5JwGSR3ROhFyZGTEiZBSjzM0SOqI1lCwcOFCJ0JKPc7HE5K64qmnnqq7BEltMjRIkqQihgZJ\nHdH6wqrM9IVVUo9zToOkjjjvvPPYs2fP5PG2bdtYu3btlHZJvcXQIKkjWidCLlq0yImQUo/z8YQk\nSSpiaJDUFUuXLq27BEltMjRI6opTTz217hIktcnQIEmSihgaJHXF97///bpLkNQmV09I6ojWd0+M\nj4/77gmpxxkaJHWESy6l+cfQIKkjWkcaDh486EiD1OOc0yBJkoo40iCpI1pHEpYtW+bjCanHOdIg\nqSv27dtXdwmS2mRokNQVTzzxRN0lSGqToUFSV0RE3SVIapOhQZIkFTE0SOqI9evXs2jRoslPZk45\nXr9+fd0lSpomV09I6ohLL72URYsWTR5v27ZtSlBwjwap9xgaJHVE65LLBQsWuORS6nE+npDUFZlZ\ndwmS2mRokCRJRQwNkiSpiKFBkiQVMTRI6oiNGzfS19c3+QGmHG/cuLHmCiVNl6snJHXEeeedx549\neyaPt23bxtq1a6e0S+othgZJHfH+97+fu+++e8q5r3zlK5N/3r17t3s1SD3GxxOSOmLFihUsXLhw\n8gNMOV6xYkXNFUqaLkcaJHWEO0JK84+hQVJHtO4IGRHuCCn1OB9PSOqI1hdWAb6wSupxjjRI6ogz\nzjiDO++8c/J4bGyMk046aUq7pN4y7ZGGiFgXEVsj4sGIeDoiBg/R570R8VBE7IuIf4qI57a0nxQR\n10bEeEQ8GhFXRcTxLX3OjIhbImJ/ROyJiL+c/o8nqS7nnXcea9eunfwAU45dcin1npmMNBwPfBf4\nNPCl1saI+Cvgz4FLgAeA/w7siIjVmXmw6vYFYDlwAfAs4Grgk8CG6h4nADuAG4A3Ay8EPhMRj2bm\nVTOoWVKXXXzxxb90btu2bVP+7GRIqbdMOzRk5nZgO0BExCG6vA14X2Zuq/r8ETAGvBr4YkSsBtYD\nA5l5R9VnI/DViLgsM/fSCA8LgTdl5pPAaEScBbwdMDRIPSAijvhmy0P/70PSXDarEyEj4teBPuCm\niXOZ+RjwHeDc6tQ5wKMTgaFyI5DAi5v63FIFhgk7gOdHxNLZrFlSZxztVdi+KlvqPbO9eqKPxi//\nsZbzY1XbRJ+fNDdm5lPAIy19DnUPmvpIkqQu6tbqiaARJtrpMzGWecT7bNq0iaVLpw5GtK4XlyTp\nWDU8PMzw8PCUc+Pj40XXznZo2Evjl/typo4UnALc0dTnlOaLIuIZwElV20Sf5S33nrimdQRiis2b\nN9Pf3z/twiVJOhYc6h/SIyMjDAwMHPXaWX08kZkP0PiFf8HEuYg4kcZchVurU7cBy6qJjRMuoBE2\nbm/q89IqTEy4ELgvM8vikCRJmlUz2afh+Ih4UUT8ZnXqOdXxqdXxx4B3R8R/iYgXAp8DfgT8A0Bm\n7qIxqfHvI+LsiHgJsAUYrlZOQGNJ5kHg0xGxJiJeD/wF8JEZ/pySJKlNM3k88VvA12nMLUh+8Yv8\ns8AbM/OKiFhCY9+FZcA3gVc07dEAcDFwJY1VE08D19NYqgk0VlxExPqqz78CDwOXZ+anZlCvJEma\nBTPZp+FmjjJCkZmXA5cfof3fqTZyOkKfu4CXTbc+SZLUGb6wSpIkFTE0SJKkIoYGSZJUxNAgSZKK\nGBokSVIRQ4MkSSpiaJAkSUUMDZIkqYihQZIkFTE0SJKkIoYGSZJUxNAgSZKKGBokSVIRQ4MkSSpi\naJAkSUUMDZIkqYihQZIkFTE0SJKkIoYGSZJUxNAgSZKKGBokSVIRQ4MkSSpiaJAkSUUMDZIkqYih\nQZIkFTE0SJKkIoYGSZJUxNAgSZKKGBokSVIRQ4MkSSpiaJAkSUUMDZIkqYihQZIkFTE0SJKkIoYG\nSZJUxNAgSZKKGBokSVIRQ4MkSSpiaJAkSUUMDZIkqYihQZIkFTE0SJKkIoYGSZJUxNAgSZKKGBok\nSVIRQ4MkSSpiaJAkSUUMDZIkqYihQZIkFTE0SJKkIoYGSZJUZNZDQ0S8JyKebvnc29S+KCI+HhEP\nR8TPIuL6iDil5R6nRsRXI+LxiNgbEVdEhAFHkqQaPbND970buACI6vjJpraPAa8AXgM8Bnwc+BKw\nDqAKB18DHgLOAX4F+DxwEHh3h+qVJElH0anQ8GRm/rT1ZEScCLwR+MPMvLk698fAaESszczbgfXA\nKuB3MvNh4K6I+GvgAxFxeWY+2XpfSZ2zb98+du3a1ZF7j4yMzOi6VatWsWTJklmuRtLRdCo0PC8i\nHgQOALcB78zMHwID1d9500THzLwvIn4AnAvcTmN04a4qMEzYAfxP4AXAnR2qWdIh7Nq1i4GBgY7c\ne6b33blzJ/39/bNcjaSj6URo+DZwKXAfsAK4HLglIv4T0AcczMzHWq4Zq9qovscO0T7RZmiQumjV\nqlXs3Lmz7fsMDAzMyn2gUZOk7pv10JCZO5oO746I24E9wOtojDwcSgBZcvujddi0aRNLly6dcm5o\naIihoaGC20tqtWTJkln7V72jA1L9hoeHGR4ennJufHy86NpOPZ6YlJnjEXE/8FzgRuBZEXFiy2jD\nKfxiNGEvcHbLbZZX360jEL9k8+bN/o9JkqTDONQ/pEdGRooeF3Z8GWNE/AfgN2ishthJYyXFBU3t\nZwC/BtxanboNeGFEnNx0mwuBceBeJPWcH/946rek3tSJfRo+FBEvjYjTIuI84Ms0gsL/rkYXPgV8\nNCLOj4gB4DPAP2fmv1S3uIFGOPh8RJwZEeuB9wFXZuYTs12vpM5rhIU0NEg9rhOPJ34V+ALwbOCn\nwLeAczLz36r2TcBTwPXAImA78NaJizPz6Yh4FY3VErcCjwNXA+/pQK2SJKlQJyZCHnHGYWb+HNhY\nfQ7X54fAq2a5NEmS1Aa3ZpYkSUUMDZIkqYihQZIkFTE0SJKkIoYGSR23eDGsWdP4ltS7Or4jpCSt\nWQP33FN3FZLa5UiDJEkqYmiQJElFDA2SJKmIoUGSJBUxNEiSpCKGBkmSVMTQIEmSihgaJHXcvffC\nC17Q+JbUuwwNkjruwIFGYDhwoO5KJLXD0CBJkooYGiRJUhFDgyRJKmJokCRJRQwNkiSpiK/Gluax\n730PfvazuquA0dGp33U74QR43vPqrkLqPYYGaZ763vfgjDPqrmKqDRvqruAX7r/f4CBNl6FBmqcm\nRhiuuQZWr663lrlkdLQRXubCCIzUawwN0jy3ejX099ddhaT5wImQkiSpiKFBkiQVMTRIkqQihgZJ\nklTE0CBJkooYGiRJUhFDgyRJKmJokCRJRQwNkiSpiKFBkiQVcRtpaZ6K/fs4i10cN0feLDlXHDcK\nZwGxfxWwpO5ypJ5iaJDmqcW7dzHCAMyhN0vOBauBEWB09054iS/lkKbD0CDNUwdOX0U/O7nWt1xO\nMToKb9gAnzp9Vd2lSD3H0CDNU3ncEu6gn/2rAf9BPWk/cAeQx9VdidR7nAgpSZKKONIgzVP79jW+\nR0bqrWOuGXViqDRjhgZpntq1q/H9p39abx1z1Qkn1F2B1HsMDdI89epXN75XrYIlNa8sHB2FDRvg\nmjkyKfOEE+B5z6u7Cqn3GBqkeerkk+FP/qTuKqZavRr6nZQp9SwnQkqSpCKGBkmSVMTQIEmSihga\nJElSEUODJEkqYmiQ1HGLF8OaNY1vSb3LJZeSOm7NGrjnnrqrkNQuRxokdcXw8HDdJUhq05wODRHx\n1oh4ICL2R8S3I+LsumuSNDOGBqn3zdnQEBGvBz4CvAc4C7gT2BERJ9damCRJx6g5GxqATcAnM/Nz\nmbkLeAuwD3hjvWVJknRsmpOhISIWAgPATRPnMjOBG4Fz66pLkqRj2VxdPXEy8AxgrOX8GPD8w1yz\nGGB0dLSDZUnHnv3797N79+627/OjH/2Ia6+9tv2CgNNPP53jjjtuVu4lacrvziMujJ6roeFwAsjD\ntJ0OsGHDhq4VI2l6/O9TmvNOB249XONcDQ0PA08By1vOn8Ivjz5M2AG8AdgNHOhYZZIkzT+LaQSG\nHUfqFI2pAnNPRHwb+E5mvq06DuAHwN9m5odqLU6SpGPQXB1pAPgo8NmI2AncTmM1xRLg6jqLkiTp\nWDVnQ0NmfrHak+G9NB5TfBdYn5k/rbcySZKOTXP28YQkSZpb5uQ+DZIkae4xNEiSpCKGBkkdExHr\nImJrRDwYEU9HxGDdNUmaOUODpE46nsYk5rdy+I3ZJPWIObt6QlLvy8ztwHaY3GtFUg9zpEGSJBUx\nNEiSpCKGBkmSVMTQIEmSihgaJElSEVdPSOqYiDgeeC4wsXLiORHxIuCRzPxhfZVJmgnfPSGpYyLi\nZcDX+eU9Gj6bmW+soSRJbTA0SJKkIs5pkCRJRQwNkiSpiKFBkiQVMTRIkqQihgZJklTE0CBJkooY\nGiRJUhFDgyRJKmJokCRJRQwNkiSpiKFBkiQV+f+6pUo6A43wfQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f066e67df90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Review length: \")\n",
    "result = map(len, X)\n",
    "plt.boxplot(result)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(nb_words=3000, seed=15)\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=500)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## D - Red LSTM con Embedding 32 y 3000 Top Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "embedding_vector_length = 32\n",
    "top_words=3000\n",
    "model = Sequential()\n",
    "model.add(Embedding(top_words, embedding_vector_length, input_length=500))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), nb_epoch=3, batch_size=64)\n",
    "model.save(\"P2-D.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000/25000 [==============================] - 184s   \n",
      "Loss: 0.354214026219   Accuracy: 0.86204\n"
     ]
    }
   ],
   "source": [
    "#Eval Model P2D\n",
    "model=load_model(\"P2-D.h5\")\n",
    "scores = model.evaluate(X_test, y_test, verbose=1)\n",
    "print \"Loss: {0}   Accuracy: {1}\".format(scores[0],scores[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## E - Red LSTM con 3000 Top Words y Embedding Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "top_words=3000\n",
    "for embedding_vector_length in [4,8,16,64,128,256]:\n",
    "    print \"EVL:\",embedding_vector_length\n",
    "    #train\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(top_words, embedding_vector_length, input_length=500))\n",
    "    model.add(LSTM(100))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    model.fit(X_train, y_train, validation_data=(X_test, y_test), nb_epoch=3, batch_size=64)\n",
    "    model.save(\"P2-E{0}.h5\".format(embedding_vector_length))\n",
    "    \n",
    "\n",
    "    #train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000/25000 [==============================] - 172s   \n",
      "Embedding Length: 4   Loss: 0.325102408178   Accuracy: 0.86124\n",
      "25000/25000 [==============================] - 183s   \n",
      "Embedding Length: 8   Loss: 0.308502162189   Accuracy: 0.8694\n",
      "25000/25000 [==============================] - 196s   \n",
      "Embedding Length: 16   Loss: 0.307057818151   Accuracy: 0.87236\n",
      "25000/25000 [==============================] - 209s   \n",
      "Embedding Length: 64   Loss: 0.484832670898   Accuracy: 0.76308\n",
      "25000/25000 [==============================] - 222s   \n",
      "Embedding Length: 128   Loss: 0.318453605099   Accuracy: 0.86652\n",
      "25000/25000 [==============================] - 206s   \n",
      "Embedding Length: 256   Loss: 0.377454812565   Accuracy: 0.84724\n"
     ]
    }
   ],
   "source": [
    "#Eval Model P2E-4-8-16-64-128-256\n",
    "for N in [4,8,16,64,128,256]:\n",
    "    model=load_model(\"P2-E{0}.h5\".format(N))\n",
    "    scores = model.evaluate(X_test, y_test, verbose=1)\n",
    "    print \"Embedding Length: {2}   Loss: {0}   Accuracy: {1}\".format(scores[0],scores[1],N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## F - Red LSTM con Embedding 32 y Top Words variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TW: 500\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/3\n",
      "25000/25000 [==============================] - 494s - loss: 0.5697 - acc: 0.6998 - val_loss: 0.4147 - val_acc: 0.8151\n",
      "Epoch 2/3\n",
      "25000/25000 [==============================] - 488s - loss: 0.4535 - acc: 0.7954 - val_loss: 0.4149 - val_acc: 0.8145\n",
      "Epoch 3/3\n",
      "25000/25000 [==============================] - 489s - loss: 0.4037 - acc: 0.8238 - val_loss: 0.4795 - val_acc: 0.7692\n",
      "TW: 1000\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/3\n",
      "25000/25000 [==============================] - 490s - loss: 0.5235 - acc: 0.7318 - val_loss: 0.4073 - val_acc: 0.8209\n",
      "Epoch 2/3\n",
      "25000/25000 [==============================] - 488s - loss: 0.3577 - acc: 0.8502 - val_loss: 0.3444 - val_acc: 0.8561\n",
      "Epoch 3/3\n",
      "25000/25000 [==============================] - 527s - loss: 0.3345 - acc: 0.8608 - val_loss: 0.3247 - val_acc: 0.8626\n",
      "TW: 2000\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/3\n",
      "25000/25000 [==============================] - 831s - loss: 0.4942 - acc: 0.7553 - val_loss: 0.4036 - val_acc: 0.8266\n",
      "Epoch 2/3\n",
      "25000/25000 [==============================] - 483s - loss: 0.3824 - acc: 0.8276 - val_loss: 0.3149 - val_acc: 0.8667\n",
      "Epoch 3/3\n",
      "25000/25000 [==============================] - 483s - loss: 0.3052 - acc: 0.8736 - val_loss: 0.3197 - val_acc: 0.8634\n",
      "TW: 4000\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/3\n",
      "25000/25000 [==============================] - 486s - loss: 0.5358 - acc: 0.7404 - val_loss: 0.3809 - val_acc: 0.8412\n",
      "Epoch 2/3\n",
      "25000/25000 [==============================] - 488s - loss: 0.3370 - acc: 0.8591 - val_loss: 0.3119 - val_acc: 0.8730\n",
      "Epoch 3/3\n",
      "25000/25000 [==============================] - 491s - loss: 0.3013 - acc: 0.8764 - val_loss: 0.3078 - val_acc: 0.8721\n",
      "TW: 5000\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/3\n",
      "25000/25000 [==============================] - 493s - loss: 0.6458 - acc: 0.6284 - val_loss: 0.8642 - val_acc: 0.5179\n",
      "Epoch 2/3\n",
      "25000/25000 [==============================] - 519s - loss: 0.5395 - acc: 0.7272 - val_loss: 0.4593 - val_acc: 0.7968\n",
      "Epoch 3/3\n",
      "25000/25000 [==============================] - 509s - loss: 0.3963 - acc: 0.8261 - val_loss: 0.3647 - val_acc: 0.8485\n",
      "TW: 10000\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/3\n",
      "25000/25000 [==============================] - 510s - loss: 0.5841 - acc: 0.6966 - val_loss: 0.4182 - val_acc: 0.8256\n",
      "Epoch 2/3\n",
      "25000/25000 [==============================] - 496s - loss: 0.3287 - acc: 0.8704 - val_loss: 0.3082 - val_acc: 0.8738\n",
      "Epoch 3/3\n",
      "25000/25000 [==============================] - 490s - loss: 0.2294 - acc: 0.9156 - val_loss: 0.4162 - val_acc: 0.8464\n"
     ]
    }
   ],
   "source": [
    "embedding_vector_length=32\n",
    "for top_words in [500,1000,2000,4000,5000,10000]:\n",
    "    (X_train, y_train), (X_test, y_test) = imdb.load_data(nb_words=top_words, seed=15)\n",
    "    X_train = sequence.pad_sequences(X_train, maxlen=500)\n",
    "    X_test = sequence.pad_sequences(X_test, maxlen=500)\n",
    "    print \"TW:\",top_words\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(top_words, embedding_vector_length, input_length=500))\n",
    "    model.add(LSTM(100))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    model.fit(X_train, y_train, validation_data=(X_test, y_test), nb_epoch=3, batch_size=64)\n",
    "    model.save(\"P2-F{0}.h5\".format(top_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000/25000 [==============================] - 199s   \n",
      "Top Words: 500   Loss: 0.479502602189   Accuracy: 0.7692\n",
      "25000/25000 [==============================] - 180s   \n",
      "Top Words: 1000   Loss: 0.324712305365   Accuracy: 0.86264\n",
      "25000/25000 [==============================] - 186s   \n",
      "Top Words: 2000   Loss: 0.319714357708   Accuracy: 0.86336\n",
      "25000/25000 [==============================] - 182s   \n",
      "Top Words: 4000   Loss: 0.307830250864   Accuracy: 0.87212\n",
      "25000/25000 [==============================] - 188s   \n",
      "Top Words: 5000   Loss: 0.36467007   Accuracy: 0.84852\n",
      "25000/25000 [==============================] - 197s   \n",
      "Top Words: 10000   Loss: 0.416235581527   Accuracy: 0.8464\n"
     ]
    }
   ],
   "source": [
    "#Eval Model P2F-500-1000-2000-5000-10000\n",
    "for N in [500,1000,2000,4000,5000,10000]:\n",
    "    (X_train, y_train), (X_test, y_test) = imdb.load_data(nb_words=N, seed=15)\n",
    "    X_train = sequence.pad_sequences(X_train, maxlen=500)\n",
    "    X_test = sequence.pad_sequences(X_test, maxlen=500)\n",
    "    model=load_model(\"P2-F{0}.h5\".format(N))\n",
    "    scores = model.evaluate(X_test, y_test, verbose=1)\n",
    "    print \"Top Words: {2}   Loss: {0}   Accuracy: {1}\".format(scores[0],scores[1],N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## G - Red LSTM + Dropout con Embedding  y Top Words variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "25000/25000 [==============================] - 425s - loss: 0.5600 - acc: 0.7071   \n",
      "Epoch 2/3\n",
      "25000/25000 [==============================] - 447s - loss: 0.3785 - acc: 0.8459   \n",
      "Epoch 3/3\n",
      "25000/25000 [==============================] - 545s - loss: 0.3432 - acc: 0.8577   \n",
      "Epoch 1/3\n",
      "25000/25000 [==============================] - 500s - loss: 0.5417 - acc: 0.7129   \n",
      "Epoch 2/3\n",
      "25000/25000 [==============================] - 475s - loss: 0.3923 - acc: 0.8320   \n",
      "Epoch 3/3\n",
      "25000/25000 [==============================] - 520s - loss: 0.3355 - acc: 0.8610   \n",
      "Epoch 1/3\n",
      "25000/25000 [==============================] - 474s - loss: 0.4616 - acc: 0.7740   \n",
      "Epoch 2/3\n",
      "25000/25000 [==============================] - 490s - loss: 0.3020 - acc: 0.8790   \n",
      "Epoch 3/3\n",
      "25000/25000 [==============================] - 526s - loss: 0.2575 - acc: 0.8961   \n",
      "Epoch 1/3\n",
      "25000/25000 [==============================] - 510s - loss: 0.4821 - acc: 0.7656   \n",
      "Epoch 2/3\n",
      "25000/25000 [==============================] - 510s - loss: 0.3118 - acc: 0.8748   \n",
      "Epoch 3/3\n",
      "25000/25000 [==============================] - 528s - loss: 0.3185 - acc: 0.8662   \n"
     ]
    }
   ],
   "source": [
    "embedding_vector_length = 32\n",
    "top_words=3000\n",
    "\n",
    "for embedding_vector_length in [8,16,64,128]:\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(top_words, embedding_vector_length, input_length=500))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(LSTM(100))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    #print(model.summary())\n",
    "    model.fit(X_train, y_train, nb_epoch=3, batch_size=64)\n",
    "    #scores = model.evaluate(X_test, y_test, verbose=1)\n",
    "    model.save(\"P2-G{0}-3000.h5\".format(embedding_vector_length))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000/25000 [==============================] - 200s   \n",
      "Embedding Length: 8   Loss: 0.324595310359   Accuracy: 0.867\n",
      "25000/25000 [==============================] - 183s   \n",
      "Embedding Length: 16   Loss: 0.414800869255   Accuracy: 0.84056\n",
      "25000/25000 [==============================] - 200s   \n",
      "Embedding Length: 64   Loss: 0.314571367285   Accuracy: 0.87568\n",
      "25000/25000 [==============================] - 190s   \n",
      "Embedding Length: 128   Loss: 0.332669987898   Accuracy: 0.87304\n"
     ]
    }
   ],
   "source": [
    "#Eval Model P2G-[8-16-64-128]-3000\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(nb_words=3000, seed=15)\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=500)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=500)\n",
    "for N in [8,16,64,128]:\n",
    "    model=load_model(\"P2-G{0}-3000.h5\".format(N))\n",
    "    scores = model.evaluate(X_test, y_test, verbose=1)\n",
    "    print \"Embedding Length: {2}   Loss: {0}   Accuracy: {1}\".format(scores[0],scores[1],N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "25000/25000 [==============================] - 447s - loss: 0.5641 - acc: 0.7095   \n",
      "Epoch 2/3\n",
      "25000/25000 [==============================] - 466s - loss: 0.4482 - acc: 0.8039   \n",
      "Epoch 3/3\n",
      "25000/25000 [==============================] - 475s - loss: 0.3940 - acc: 0.8355   \n",
      "Epoch 1/3\n",
      "25000/25000 [==============================] - 463s - loss: 0.5826 - acc: 0.6768   \n",
      "Epoch 2/3\n",
      "25000/25000 [==============================] - 457s - loss: 0.4192 - acc: 0.8139   \n",
      "Epoch 3/3\n",
      "25000/25000 [==============================] - 461s - loss: 0.3580 - acc: 0.8472   \n",
      "Epoch 1/3\n",
      "25000/25000 [==============================] - 427s - loss: 0.5282 - acc: 0.7272   \n",
      "Epoch 2/3\n",
      "25000/25000 [==============================] - 406s - loss: 0.3257 - acc: 0.8681   \n",
      "Epoch 3/3\n",
      "25000/25000 [==============================] - 401s - loss: 0.2785 - acc: 0.8884   \n",
      "Epoch 1/3\n",
      "25000/25000 [==============================] - 394s - loss: 0.6240 - acc: 0.6582   \n",
      "Epoch 2/3\n",
      "25000/25000 [==============================] - 387s - loss: 0.4201 - acc: 0.8134   \n",
      "Epoch 3/3\n",
      "25000/25000 [==============================] - 386s - loss: 0.3163 - acc: 0.8711   \n",
      "Epoch 1/3\n",
      "25000/25000 [==============================] - 379s - loss: 0.6440 - acc: 0.6027   \n",
      "Epoch 2/3\n",
      "25000/25000 [==============================] - 381s - loss: 0.3942 - acc: 0.8274   \n",
      "Epoch 3/3\n",
      "25000/25000 [==============================] - 379s - loss: 0.2903 - acc: 0.8826   \n"
     ]
    }
   ],
   "source": [
    "embedding_vector_length=32\n",
    "for top_words in [1000,2000,3000,4000,5000]:\n",
    "    (X_train, y_train), (X_test, y_test) = imdb.load_data(nb_words=top_words, seed=15)\n",
    "    X_train = sequence.pad_sequences(X_train, maxlen=500)\n",
    "    X_test = sequence.pad_sequences(X_test, maxlen=500)\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(top_words, embedding_vector_length, input_length=500))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(LSTM(100))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    #print(model.summary())\n",
    "    model.fit(X_train, y_train, nb_epoch=3, batch_size=64)\n",
    "    #scores = model.evaluate(X_test, y_test, verbose=1)\n",
    "    model.save(\"P2-G32-{0}.h5\".format(top_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000/25000 [==============================] - 189s   \n",
      "Top Words: 1000   Loss: 0.400974370446   Accuracy: 0.83472\n",
      "25000/25000 [==============================] - 188s   \n",
      "Top Words: 2000   Loss: 0.337027104659   Accuracy: 0.8606\n",
      "25000/25000 [==============================] - 190s   \n",
      "Top Words: 3000   Loss: 0.309789932151   Accuracy: 0.86924\n",
      "25000/25000 [==============================] - 203s   \n",
      "Top Words: 4000   Loss: 0.321982482461   Accuracy: 0.86596\n",
      "25000/25000 [==============================] - 198s   \n",
      "Top Words: 5000   Loss: 0.335232408721   Accuracy: 0.85232\n"
     ]
    }
   ],
   "source": [
    "#Eval Model P2G-32-[500-1000-2000-5000-10000]\n",
    "\n",
    "for N in [1000,2000,3000,4000,5000]:\n",
    "    (X_train, y_train), (X_test, y_test) = imdb.load_data(nb_words=N, seed=15)\n",
    "    X_train = sequence.pad_sequences(X_train, maxlen=500)\n",
    "    X_test = sequence.pad_sequences(X_test, maxlen=500)\n",
    "    model=load_model(\"P2-G32-{0}.h5\".format(N))\n",
    "    scores = model.evaluate(X_test, y_test, verbose=1)\n",
    "    print \"Top Words: {2}   Loss: {0}   Accuracy: {1}\".format(scores[0],scores[1],N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
